{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoChat Lite on Colab: Cloud-Based Video Labeling\n",
    "\n",
    "This notebook demonstrates the core \"behind-the-scenes\" logic of the VideoChat application's **lite mode**, adapted to run directly in Google Colab. Instead of using local, GPU-intensive models, this workflow relies entirely on Google Cloud APIs (Google AI Studio or Vertex AI) for analysis.\n",
    "\n",
    "We will walk through:\n",
    "1.  **Environment Setup**: Cloning the application code and installing necessary packages.\n",
    "2.  **Authentication & Configuration**: Securely providing API keys and defining the target video.\n",
    "3.  **Asset Preparation**: Programmatically downloading and preparing a video, just like the web app does.\n",
    "4.  **Cloud-Based Labeling**: Calling the function that sends the video and context to the selected Google Cloud model for analysis.\n",
    "5.  **Result Processing**: Displaying the structured JSON labels returned by the model.\n",
    "\n",
    "**Requirements**:\n",
    "*   A Google Account to run this Colab notebook.\n",
    "*   A Google AI Studio API Key or a configured Google Cloud Platform (GCP) project for Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll clone the project repository from GitHub to get access to all the necessary Python helper scripts (`app.py`, `inference_logic.py`, etc.). Then, we'll install the required packages listed in `requirements-lite.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repository URL)\n",
    "!git clone https://github.com/username/vChat.git\n",
    "%cd vChat\n",
    "\n",
    "# Install dependencies using pip\n",
    "!pip install -r requirements-lite.txt\n",
    "\n",
    "# Add the cloned repository directory to Python's path to allow imports\n",
    "import sys\n",
    "import os\n",
    "if '/content/vChat' not in sys.path:\n",
    "    sys.path.append('/content/vChat')\n",
    "\n",
    "# Now we can import the necessary functions from the application\n",
    "import asyncio\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from app import prepare_video_assets_async, generate_and_save_croissant_metadata\n",
    "from inference_logic import run_gemini_labeling_pipeline, run_vertex_labeling_pipeline\n",
    "from factuality_logic import parse_vtt\n",
    "\n",
    "# This allows running async code in a Jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Setup complete. You can now proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication & Configuration\n",
    "\n",
    "Now, let's configure the model and provide the necessary credentials.\n",
    "\n",
    "**‚û°Ô∏è Action Required**: \n",
    "1. Choose which cloud model to use (`MODEL_SELECTION`).\n",
    "2. Fill in the `VIDEO_URL`.\n",
    "3. Provide credentials for your chosen model:\n",
    "    *   **For Gemini**: Click the **Key icon (üîë)** in the left sidebar, add a new secret named `GEMINI_API_KEY`, and paste your key there. Get one from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "    *   **For Vertex AI**: You will be prompted to log in to your Google account when you run the cell. Make sure to fill in your `VERTEX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata, auth\n",
    "\n",
    "# --- USER CONFIGURATION ---\n",
    "\n",
    "# Choose which cloud model to use: 'gemini' or 'vertex'\n",
    "MODEL_SELECTION = 'gemini'\n",
    "\n",
    "# Provide the URL of the video you want to analyze\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=Ad_TEk94B9w\" # Example: A cat playing\n",
    "\n",
    "# --- Credentials (handled by Colab) ---\n",
    "\n",
    "# 1. For Google AI Studio ('gemini') - Fetches from Colab Secrets\n",
    "GEMINI_API_KEY = None\n",
    "GEMINI_MODEL_NAME = \"models/gemini-1.5-pro-latest\"\n",
    "if MODEL_SELECTION == 'gemini':\n",
    "    try:\n",
    "        GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "        print(\"Successfully loaded Gemini API Key from Colab Secrets.\")\n",
    "    except userdata.SecretNotFoundError:\n",
    "        print(\"ERROR: Gemini API Key not found. Please add it to Colab Secrets (üîë).\")\n",
    "\n",
    "# 2. For Google Cloud Vertex AI ('vertex')\n",
    "VERTEX_PROJECT_ID = \"your-gcp-project-id-here\"  # <-- IMPORTANT: SET YOUR GCP PROJECT ID\n",
    "VERTEX_LOCATION = \"us-central1\"\n",
    "VERTEX_MODEL_NAME = \"gemini-1.5-pro-preview-0409\"\n",
    "if MODEL_SELECTION == 'vertex':\n",
    "    print(\"Authenticating for Vertex AI... Please follow the pop-up prompt.\")\n",
    "    auth.authenticate_user()\n",
    "    print(\"‚úÖ Vertex AI authentication successful.\")\n",
    "\n",
    "print(\"\\nConfiguration loaded. Ready to run the analysis pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Labeling Pipeline Function\n",
    "\n",
    "The function below encapsulates the entire end-to-end process. It mirrors the exact steps the full web application takes when you click the \"Generate & Append Labels\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_full_labeling_pipeline():\n",
    "    \"\"\"Main async function to orchestrate the labeling process.\"\"\"\n",
    "    final_labels = None\n",
    "    csv_row_data = None\n",
    "\n",
    "    # --- Step 1: Prepare Video Assets ---\n",
    "    print(f\"\\n--- [Step 1] Preparing Assets for: {VIDEO_URL} ---\")\n",
    "    paths = await prepare_video_assets_async(VIDEO_URL)\n",
    "    video_path = paths.get(\"video\")\n",
    "    transcript_path = paths.get(\"transcript\")\n",
    "    metadata = paths.get(\"metadata\", {})\n",
    "    print(f\"  -> Video processed and saved to: {video_path}\")\n",
    "    print(f\"  -> Transcript found at: {transcript_path}\")\n",
    "    print(f\"  -> Extracted Metadata: {metadata}\")\n",
    "\n",
    "    # --- Step 2: Prepare Context for the Model ---\n",
    "    print(\"\\n--- [Step 2] Preparing Textual Context ---\")\n",
    "    caption = metadata.get(\"caption\", \"No caption available.\")\n",
    "    transcript_text = \"No transcript available.\" \n",
    "    if transcript_path and Path(transcript_path).exists():\n",
    "        transcript_text = parse_vtt(transcript_path)\n",
    "    print(\"  -> Caption and transcript are ready.\")\n",
    "\n",
    "    # --- Step 3: Run Cloud-Based Labeling ---\n",
    "    print(f\"\\n--- [Step 3] Running Labeling with '{MODEL_SELECTION.capitalize()}' Model ---\")\n",
    "    \n",
    "    pipeline_generator = None\n",
    "    if MODEL_SELECTION == 'gemini':\n",
    "        if not GEMINI_API_KEY: \n",
    "            print(\"ERROR: Cannot proceed without a Gemini API Key.\")\n",
    "            return\n",
    "        gemini_config = {\"api_key\": GEMINI_API_KEY, \"model_name\": GEMINI_MODEL_NAME}\n",
    "        pipeline_generator = run_gemini_labeling_pipeline(video_path, caption, transcript_text, gemini_config, include_comments=True)\n",
    "    elif MODEL_SELECTION == 'vertex':\n",
    "        vertex_config = {\"project_id\": VERTEX_PROJECT_ID, \"location\": VERTEX_LOCATION, \"model_name\": VERTEX_MODEL_NAME, \"api_key\": None}\n",
    "        pipeline_generator = run_vertex_labeling_pipeline(video_path, caption, transcript_text, vertex_config, include_comments=True)\n",
    "    else:\n",
    "        print(f\"ERROR: Invalid model selection '{MODEL_SELECTION}'. Choose 'gemini' or 'vertex'.\")\n",
    "        return\n",
    "\n",
    "    # The pipeline yields progress messages and finally the result dictionary\n",
    "    async for message in pipeline_generator:\n",
    "        if isinstance(message, dict): # This is the final result\n",
    "            final_labels = message\n",
    "        elif isinstance(message, str): # This is a progress update\n",
    "            print(f\"  -> {message.strip()}\")\n",
    "\n",
    "    if not final_labels:\n",
    "        print(\"\\nERROR: Failed to retrieve labels from the model.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 4: Display Parsed Results ---\n",
    "    print(\"\\n--- [Step 4] Successfully Parsed JSON Labels ---\")\n",
    "    pprint.pprint(final_labels)\n",
    "    \n",
    "    # --- Step 5: Generate Croissant Metadata ---\n",
    "    print(\"\\n--- [Step 5] Generating Croissant Metadata File ---\")\n",
    "    # Recreate the data structure expected by the metadata function\n",
    "    def get_score(value):\n",
    "        return value.get('score', '') if isinstance(value, dict) else value\n",
    "\n",
    "    disinfo_analysis = final_labels.get(\"disinformation_analysis\", {})\n",
    "    sentiment_tactics = disinfo_analysis.get(\"sentiment_and_bias_tactics\", {})\n",
    "    \n",
    "    csv_row_data = {\n",
    "        \"id\": metadata.get(\"id\", \"\"),\n",
    "        \"link\": metadata.get(\"link\", VIDEO_URL),\n",
    "        \"caption\": caption,\n",
    "        \"videocontext\": final_labels.get(\"video_context_summary\", \"\"),\n",
    "        \"politicalbias\": get_score(final_labels.get(\"political_bias\", \"\")),\n",
    "        \"criticism\": get_score(final_labels.get(\"criticism_level\", \"\")),\n",
    "        \"videoaudiopairing\": get_score(final_labels.get(\"video_audio_pairing\", \"\")),\n",
    "        \"videocaptionpairing\": get_score(final_labels.get(\"video_caption_pairing\", \"\")),\n",
    "        \"audiocaptionpairing\": get_score(final_labels.get(\"audio_caption_pairing\", \"\")),\n",
    "        \"disinfo_level\": disinfo_analysis.get(\"disinformation_level\", \"\"),\n",
    "        \"disinfo_intent\": disinfo_analysis.get(\"disinformation_intent\", \"\"),\n",
    "        \"disinfo_threat_vector\": disinfo_analysis.get(\"threat_vector\", \"\"),\n",
    "        \"disinfo_emotional_charge\": sentiment_tactics.get(\"emotional_charge\", \"\"),\n",
    "    }\n",
    "    \n",
    "    metadata_path = await generate_and_save_croissant_metadata(csv_row_data)\n",
    "    print(f\"  -> Metadata file saved to: {metadata_path}\")\n",
    "    print(\"\\n--- Pipeline Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Pipeline\n",
    "\n",
    "Now, we execute the main function. This will trigger the video download, API calls, and processing. You will see real-time progress updates printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(run_full_labeling_pipeline())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}