{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72afd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from 'C:\\Users\\kevin\\Documents\\GitHub\\open-vRAG\\sentiment-analysis\\processed.csv'...\n",
      "Unique classes: [-1  1]\n",
      "Training set size: 1280000\n",
      "Testing set size: 320000\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"autogluon.tabular==1.0.0\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.metrics import classification_report\n",
    "import emoji\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "csv_path = \"C:\\\\Users\\\\kevin\\\\Documents\\\\GitHub\\\\open-vRAG\\\\sentiment-analysis\\\\processed.csv\"\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\"ERROR: File '{csv_path}' not found.\")\n",
    "    print(\"Please run 'preprocess.py' first.\")\n",
    "    import sys\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Loading dataset from '{csv_path}'...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming your text column is named 'text'\n",
    "text_col = 'text'  # change to your actual column name\n",
    "\n",
    "# --- Full cleaning function ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 1️⃣ Collapse repeated USER mentions\n",
    "    text = re.sub(r'\\bUSER\\b(?:\\s+\\bUSER\\b)+', 'USER', text)\n",
    "    \n",
    "    # 2️⃣ Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 3️⃣ Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    # 4️⃣ Remove unnecessary punctuation (except basic sentence punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    \n",
    "    # 5️⃣ Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6️⃣ Lowercase all text\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- Apply cleaning ---\n",
    "df[text_col] = df[text_col].apply(clean_text)\n",
    "\n",
    "\n",
    "# --- 2. Define Label ---\n",
    "label_col = 'sentiment_3class'\n",
    "print(f\"Unique classes: {df[label_col].unique()}\")\n",
    "\n",
    "# Note: Data is already cleaned by preprocess.py\n",
    "data = df\n",
    "\n",
    "# --- 3. Split Data ---\n",
    "train_df, test_df = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    stratify=data[label_col]\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b07f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv_path = \"C:\\\\Users\\\\kevin\\\\Documents\\\\GitHub\\\\open-vRAG\\\\sentiment-analysis\\\\processed_cleaned.csv\"\n",
    "\n",
    "df.to_csv(cleaned_csv_path, index=False)  # index=False avoids saving row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b8880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing model directory: AutogluonModels/sentiment-analysis_tabular\n"
     ]
    }
   ],
   "source": [
    "save_path = 'AutogluonModels/sentiment-analysis_tabular'\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Removing existing model directory: {save_path}\")\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label= 'sentiment_3class',\n",
    "    eval_metric='accuracy',\n",
    "    problem_type='binary',\n",
    "    path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79185945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.1\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          32\n",
      "Memory Avail:       10.62 GB / 31.74 GB (33.4%)\n",
      "Disk Space Avail:   22.58 GB / 464.32 GB (4.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Beginning AutoGluon training ... Time limit = 1200s\n",
      "AutoGluon will save models to \"c:\\Users\\kevin\\Documents\\GitHub\\open-vRAG\\sentiment-analysis\\AutogluonModels\\sentiment-analysis_tabular\"\n",
      "Train Data Rows:    1280000\n",
      "Train Data Columns: 1\n",
      "Label Column:       sentiment_3class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = -1\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1) vs negative (-1) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10983.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 141.98 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 10000 to 306 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', ['binned', 'text_special']) :  12 | ['text.char_count', 'text.word_count', 'text.lower_ratio', 'text.digit_ratio', 'text.special_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 307 | ['__nlp__.about', '__nlp__.after', '__nlp__.again', '__nlp__.all', '__nlp__.all the', ...]\n",
      "\t312.3s = Fit runtime\n",
      "\t1 features in original data used to generate 320 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 766.60 MB (6.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 314.96s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 1267200, Val Rows: 12800\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'LR': [{}],\n",
      "\t'RF': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'XT': [{}],\n",
      "}\n",
      "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: RandomForest ... Training model for up to 885.04s of the 885.03s of remaining time.\n",
      "\tFitting with cpus=32, gpus=0, mem=0.8/9.3 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 109 due to low time. Expected time usage reduced from 2328.2s -> 851.8s...\n",
      "\t0.7439\t = Validation score   (accuracy)\n",
      "\t293.3s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTrees ... Training model for up to 591.43s of the 591.43s of remaining time.\n",
      "\tFitting with cpus=32, gpus=0, mem=0.8/9.6 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 217 due to low time. Expected time usage reduced from 813.3s -> 588.9s...\n",
      "\t0.7448\t = Validation score   (accuracy)\n",
      "\t405.74s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 184.92s of the 184.91s of remaining time.\n",
      "\tFitting with cpus=24, gpus=0, mem=5.8/10.5 GB\n",
      "\t0.7562\t = Validation score   (accuracy)\n",
      "\t77.18s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LinearModel ... Training model for up to 107.45s of the 107.44s of remaining time.\n",
      "\tFitting with cpus=32, gpus=0, mem=3.0/10.4 GB\n",
      "\tWarning: Exception caused LinearModel to fail during training... Skipping this model.\n",
      "\t\tUnable to allocate 2.94 GiB for an array with shape (1267200, 311) and data type int64\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\lr\\lr_model.py\", line 170, in _fit\n",
      "    X = self.preprocess(X, is_train=True)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 550, in preprocess\n",
      "    X = self._preprocess(X, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\lr\\lr_model.py\", line 113, in _preprocess\n",
      "    X = self._preprocess_train(X, feature_types, self.params[\"vectorizer_dict_size\"])\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\lr\\lr_model.py\", line 155, in _preprocess_train\n",
      "    return self._pipeline.fit_transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 996, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 897, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 719, in fit_transform\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 894, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py\", line 469, in fit\n",
      "    self.statistics_ = self._dense_fit(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py\", line 551, in _dense_fit\n",
      "    median_masked = np.ma.median(masked_X, axis=0)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\extras.py\", line 790, in median\n",
      "    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 3894, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\extras.py\", line 809, in _median\n",
      "    asorted = sort(a, axis=axis, fill_value=fill_value)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py\", line 7302, in sort\n",
      "    a.sort(axis=axis, kind=kind, order=order, endwith=endwith,\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py\", line 5901, in sort\n",
      "    sidx = self.argsort(axis=axis, kind=kind, order=order,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py\", line 5725, in argsort\n",
      "    return filled.argsort(axis=axis, kind=kind, order=order)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 2.94 GiB for an array with shape (1267200, 311) and data type int64\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 91.92s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.7562\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1126.06s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 84184.9 rows/s (12800 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.7562\n",
      "\tBest Threshold: 0.500\t| val: 0.7562\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\kevin\\Documents\\GitHub\\open-vRAG\\sentiment-analysis\\AutogluonModels\\sentiment-analysis_tabular\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2a83c988ec0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(\n",
    "    train_df,\n",
    "    time_limit=1200, # Set a shorter limit for simpler models (5 min)\n",
    "    presets='medium_quality_faster_train', # Use a preset suitable for tabular data\n",
    "    \n",
    "    # We use included_model_types to specify only the simple models we want.\n",
    "    hyperparameters={\n",
    "        'LR': {},          # Logistic Regression\n",
    "        'RF': {},          # Random Forest\n",
    "        'XGB': {},         # XGBoost\n",
    "        'XT': {},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55573980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Leaderboard (All Trained Models) ---\n",
      "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0              XGBoost    0.751788   0.756172    accuracy        9.816408   \n",
      "1  WeightedEnsemble_L2    0.751788   0.756172    accuracy        9.853835   \n",
      "2           ExtraTrees    0.742425   0.744766    accuracy        2.992769   \n",
      "3         RandomForest    0.740112   0.743906    accuracy        1.795156   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0       0.149039   77.180781                 9.816408                0.149039   \n",
      "1       0.152046   77.208922                 0.037427                0.003008   \n",
      "2       0.165528  405.739315                 2.992769                0.165528   \n",
      "3       0.079916  293.299355                 1.795156                0.079916   \n",
      "\n",
      "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0          77.180781            1       True          3  \n",
      "1           0.028141            2       True          4  \n",
      "2         405.739315            1       True          2  \n",
      "3         293.299355            1       True          1  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Leaderboard (All Trained Models) ---\")\n",
    "leaderboard = predictor.leaderboard(test_df)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e590971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
