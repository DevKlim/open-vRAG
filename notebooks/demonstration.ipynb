{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoChat Inference Demonstration\n",
    "\n",
    "This notebook demonstrates the core \"behind-the-scenes\" logic of the VideoChat application. We will walk through:\n",
    "1.  **Setup**: Loading the necessary model and processor from Hugging Face.\n",
    "2.  **Input Preparation**: Defining the video path and the question to ask.\n",
    "3.  **Inference Function**: A look at the reusable function that runs a single pass of the model.\n",
    "4.  **Multi-Perception Loop**: The iterative process where the model analyzes the video in steps, using context from previous steps to refine its focus.\n",
    "\n",
    "**Hardware Requirement**: This notebook requires a CUDA-enabled NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the src directory to the system path to import modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from my_vision_process import process_vision_info, client # Assuming client is None for local execution\n",
    "\n",
    "# Check for GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"Error: CUDA is not available. This notebook requires a GPU.\")\n",
    "print(f\"CUDA is available. Using device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load Model and Processor\n",
    "\n",
    "load the `OpenGVLab/VideoChat-R1_5` model and its associated processor. We use `bfloat16` for memory efficiency and `flash_attention_2` for faster performance if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    \"\"\"Loads and returns the model and processor onto the GPU.\"\"\"\n",
    "    model_path = \"OpenGVLab/VideoChat-R1_5\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "        print(\"flash-attn is available, using 'flash_attention_2'.\")\n",
    "    except ImportError:\n",
    "        print(\"flash-attn not installed. Falling back to 'sdpa'.\")\n",
    "        attn_implementation = \"sdpa\"\n",
    "\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=attn_implementation\n",
    "    ).eval()\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    print(\"Model and processor loaded successfully.\")\n",
    "    return model, processor\n",
    "\n",
    "model, processor = setup_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Preparation\n",
    "\n",
    "Define the path to your video and the question you want to ask. We also define the prompt templates used by the application. The `GLUE` prompt asks the model to identify relevant time clips, which are used in subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTANT --- \n",
    "# Change this to the path of a video file on your system.\n",
    "# For example: \"videos/my_cool_video.mp4\"\n",
    "VIDEO_PATH = \"path/to/your/video.mp4\"\n",
    "QUESTION = \"What is the person in the video doing?\"\n",
    "NUM_PERCEPTIONS = 3\n",
    "\n",
    "if not os.path.exists(VIDEO_PATH):\n",
    "    print(f\"ERROR: Video file not found at '{VIDEO_PATH}'. Please update the VIDEO_PATH variable.\")\n",
    "\n",
    "QA_THINK_GLUE = \"\"\"Answer the question: \"[QUESTION]\" according to the content of the video. \\n\n",
    "Output your think process within the <think> </think> tags.\\n\\nThen, provide your answer within the <answer> </answer> tags. At the same time, in the <glue> </glue> tags, present the precise time period in seconds of the video clips on which you base your answer in the format of [(s1, e1), (s2, e2), ...]. For example: <think>...</think><answer>A</answer><glue>[(5.2, 10.4)]</glue>.\"\"\"\n",
    "\n",
    "QA_THINK = \"\"\"Answer the question: \"[QUESTION]\" according to the content of the video.\\n\\nOutput your think process within the <think> </think> tags.\\n\\nThen, provide your answer within the <answer> </answer> tags. For example: <think>...</think><answer>A</answer>.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Core Inference Function\n",
    "\n",
    "This function encapsulates a single interaction with the model. It takes the video, a prompt, and optional `pred_glue` (time clips from a previous step), processes them, and returns the model's raw text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(video_path, prompt, model, processor, pred_glue=None, max_new_tokens=2048):\n",
    "    \"\"\"Runs a single inference pass on the model.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"video\", \n",
    "                \"video\": video_path,\n",
    "                'key_time': pred_glue,\n",
    "                \"total_pixels\": 128*12 * 28 * 28, \n",
    "                \"min_pixels\": 128 * 28 * 28,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True, client=client)\n",
    "    fps_inputs = video_kwargs['fps'][0]\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, fps=fps_inputs, padding=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "\n",
    "    generated_ids = [output_ids[i][len(inputs.input_ids[i]):] for i in range(len(output_ids))]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Multi-Perception Loop\n",
    "\n",
    "This is the main logic. We loop for `NUM_PERCEPTIONS` iterations. In each step (except the last), we use the `GLUE` prompt to ask for relevant time clips. We extract these clips and feed them back into the next iteration via the `pred_glue` argument, allowing the model to focus its attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "pred_glue = None\n",
    "\n",
    "print(f\"--- Starting Inference for '{VIDEO_PATH}' ---\")\n",
    "\n",
    "for perception in range(NUM_PERCEPTIONS):\n",
    "    print(f\"\\n>>> Perception Iteration {perception + 1}/{NUM_PERCEPTIONS} <<<\")\n",
    "\n",
    "    # Use the GLUE prompt for all but the last iteration\n",
    "    if perception < NUM_PERCEPTIONS - 1:\n",
    "        current_prompt = QA_THINK_GLUE.replace(\"[QUESTION]\", QUESTION)\n",
    "    else:\n",
    "        current_prompt = QA_THINK.replace(\"[QUESTION]\", QUESTION)\n",
    "    \n",
    "    print(f\"[Prompt]:\\n{current_prompt}\\n\")\n",
    "    if pred_glue:\n",
    "        print(f\"[Input Glue]: Focusing on time clips {pred_glue}\\n\")\n",
    "\n",
    "    # Run inference\n",
    "    ans = inference(\n",
    "        VIDEO_PATH, current_prompt, model, processor,\n",
    "        pred_glue=pred_glue\n",
    "    )\n",
    "\n",
    "    print(f\"[Model Raw Output]:\\n{ans}\\n\")\n",
    "    answers.append(ans)\n",
    "    \n",
    "    # Reset glue and try to parse it from the latest answer\n",
    "    pred_glue = None\n",
    "    try:\n",
    "        pattern_glue = r'<glue>(.*?)</glue>'\n",
    "        match_glue = re.search(pattern_glue, ans, re.DOTALL)\n",
    "        if match_glue:\n",
    "            glue_str = match_glue.group(1).strip()\n",
    "            pred_glue = ast.literal_eval(glue_str)\n",
    "            print(f\"[Output Glue Found]: Extracted {pred_glue} for next iteration.\")\n",
    "        else:\n",
    "            print(\"[Output Glue Found]: None.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse glue from output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Answer\n",
    "\n",
    "The final answer is the output from the last iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = answers[-1] if answers else \"No answer was generated.\"\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
