{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5fb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm  # progress bar in Jupyter\n",
    "\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio, asyncio\n",
    "from twscrape import API\n",
    "from twscrape import gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f182d",
   "metadata": {},
   "source": [
    "## Scraping basic video information in X\n",
    "\n",
    "start from simple Format\n",
    "\n",
    "twitterlink | # likes | # shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c250215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cookies_to_string(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts Netscape 'cookies.txt' or a JSON list of cookies.\n",
    "    Returns a single 'name=value; name2=value2; ...' cookie string.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Cookie file not found: {p.resolve()}\")\n",
    "\n",
    "    text = p.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "    # Heuristic: JSON or Netscape?\n",
    "    if text.startswith(\"[\") or text.startswith(\"{\"):\n",
    "        # JSON format (list of dicts with 'name' and 'value')\n",
    "        data = json.loads(text)\n",
    "        # Some exporters wrap in {\"cookies\": [...]}:\n",
    "        if isinstance(data, dict) and \"cookies\" in data:\n",
    "            data = data[\"cookies\"]\n",
    "        pairs = []\n",
    "        for c in data:\n",
    "            name = c.get(\"name\")\n",
    "            value = c.get(\"value\")\n",
    "            if name and value is not None:\n",
    "                pairs.append(f\"{name}={value}\")\n",
    "        return \"; \".join(pairs)\n",
    "\n",
    "    # Netscape cookies.txt format (tab-separated columns)\n",
    "    # Skip comments (#...), keep last 2 columns as name/value\n",
    "    pairs = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        # Netscape spec: domain, flag, path, secure, expiration, name, value\n",
    "        if len(parts) >= 7:\n",
    "            name, value = parts[-2], parts[-1]\n",
    "            if name and value is not None:\n",
    "                pairs.append(f\"{name}={value}\")\n",
    "    if not pairs:\n",
    "        raise ValueError(\"No cookies parsed. Make sure the file is for x.com and not empty.\")\n",
    "    return \"; \".join(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf611b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08843bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def init_api(cookies_path=\"cookies.txt\", db_path=\"accounts.db\", label=\"my_cookie_account\"):\n",
    "    cookie_str = load_cookies_to_string(cookies_path)\n",
    "    api = API(db_path)  # creates/uses a local sqlite db for sessions\n",
    "    # Add (or replace) an account backed by cookies\n",
    "    # username/password placeholders are fine when cookies are provided\n",
    "    try:\n",
    "        # If account label already exists, remove then add (keeps this idempotent)\n",
    "        acc = await api.pool.get_account(label)\n",
    "        await api.pool.delete_account(label)\n",
    "    except Exception:\n",
    "        pass\n",
    "    await api.pool.add_account(label, \"-\", \"-\", \"-\", cookies=cookie_str)\n",
    "    await api.pool.login_all()  # validate/refresh session\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = asyncio.run(init_api(\"cookies.txt\"))  # or \"cookies.json\" if that's what you exported\n",
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81de74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize your search here:\n",
    "QUERY = 'has:videos -is:retweet -is:reply lang:en'   \n",
    "LIMIT = 5      \n",
    "DELAY = 2\n",
    "PRODUCT = \"Media\"\n",
    "OUT_CSV = \"videos_twscrape.csv\"\n",
    "\n",
    "def iter_media(media_obj):\n",
    "    \"\"\"Yield media objects regardless of whether tweet.media is single, list, or has .all.\"\"\"\n",
    "    if not media_obj:\n",
    "        return\n",
    "    # Already an iterable (list/tuple/etc.)\n",
    "    if hasattr(media_obj, \"__iter__\") and not isinstance(media_obj, (str, bytes)):\n",
    "        # Some twscrape versions expose .all; prefer it if present\n",
    "        if hasattr(media_obj, \"all\") and media_obj.all:\n",
    "            for m in media_obj.all:\n",
    "                yield m\n",
    "        else:\n",
    "            for m in media_obj:\n",
    "                yield m\n",
    "        return\n",
    "    # Has a .all attribute (collection wrapper)\n",
    "    if hasattr(media_obj, \"all\") and media_obj.all:\n",
    "        for m in media_obj.all:\n",
    "            yield m\n",
    "        return\n",
    "    # Fallback: single media object\n",
    "    yield media_obj\n",
    "\n",
    "def has_video_media(tweet):\n",
    "    for m in iter_media(getattr(tweet, \"media\", None)):\n",
    "        if getattr(m, \"type\", \"\") in (\"video\", \"animated_gif\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "async def search_videos_to_csv(api, query=QUERY, limit=LIMIT, product=PRODUCT, out_csv=OUT_CSV):\n",
    "    tweets = await gather(api.search(query, limit=limit, kv={\"product\": product}))\n",
    "\n",
    "    rows = []\n",
    "    for tw in tweets:\n",
    "        if not has_video_media(tw):\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"twitterlink\": f\"https://x.com/{tw.user.username}/status/{tw.id}\",\n",
    "            \"username\": tw.user.username,\n",
    "            \"content\": tw.content,\n",
    "            \"date\": tw.date,\n",
    "            \"likes\": tw.likeCount,\n",
    "            \"shares\": tw.retweetCount,\n",
    "            \"replies\": tw.replyCount,\n",
    "            \"tweet_id\": tw.id,  # handy for dedup\n",
    "        })\n",
    "        time.sleep(DELAY)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No matching tweets with video found.\")\n",
    "        return\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(df)} rows to {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4822477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "awaitable = search_videos_to_csv(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8236305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching tweets with video found.\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(awaitable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6961f",
   "metadata": {},
   "source": [
    "## Above does not work\n",
    "#### snscrape libaray: outdated for twitter post\n",
    "#### twscrape library: did not figure out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a292ce",
   "metadata": {},
   "source": [
    "## Decide to collect video post mannuelly:\n",
    "\n",
    "#### Procedure\n",
    "1. look for video post\n",
    "\n",
    "2. download video through `TwitterVideoDownloader`\n",
    "\n",
    "3. extract audio through `Restream` (Has hourly limit)\n",
    "\n",
    "4. record down links, captions, likes, shares, post_time, collect_time, transcription in a `twitterVideo.csv`\n",
    "\n",
    "5. video/audio files are uploaded to Google Drive\n",
    "\n",
    "6. transcription files are stored in transcript folder \n",
    "\n",
    "\n",
    "Transcription extraction basing on whisper from openai:\n",
    "\n",
    "- Using `TurboScribe` (Limited Daily Free times)\n",
    "    - setting{Max Words Per Segment:8, Max Duration Per Segment (Seconds): 10, Max Characters Per Segment: 80}\n",
    "    - in VTT format\n",
    "\n",
    "- send it to whisper using API (NOT FREE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
